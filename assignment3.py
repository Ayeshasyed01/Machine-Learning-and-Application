# -*- coding: utf-8 -*-
"""Assignment3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10CzhtF9LgdVPE_gV80TvM_lwQRHvyxcl
"""

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.optimizers import Adam, SGD

from sklearn.metrics import classification_report, confusion_matrix

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Load the CIFAR-10 dataset
(X_train, y_train), (X_test, y_test) = cifar10.load_data()

# Preprocess the data
X_train = X_train.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0
y_train = to_categorical(y_train, 10)
y_test = to_categorical(y_test, 10)

"""Model-1"""

# Define the modified LeNet-5 model for CIFAR-10
model = models.Sequential([
    layers.Input(shape=(32, 32, 3)),  # Adjusted to the correct input shape
    layers.Conv2D(6, kernel_size=(5, 5), activation='tanh', padding='same'),  # Padding added to maintain dimension
    layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2)),
    layers.Conv2D(16, kernel_size=(5, 5), activation='tanh'),
    layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2)),
    layers.Conv2D(120, kernel_size=(5, 5), activation='tanh'),
    layers.Flatten(),
    layers.Dense(84, activation='tanh'),
    layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Assuming X_train, y_train, X_test, and y_test are already loaded and preprocessed for CIFAR-10
# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=128, validation_data=(X_test, y_test))

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)
print("Test accuracy:", test_acc)

plt.figure(figsize=(12, 4))
plt.subplot(121)
plt.plot(history.history['loss'], label='training')
plt.plot(history.history['val_loss'], label='validation')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()

plt.subplot(122)
plt.plot(history.history['accuracy'], label='training')
plt.plot(history.history['val_accuracy'], label='validation')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend()

plt.show()

y_true = np.argmax(y_test, axis=1)
y_pred = np.argmax(model.predict(X_test), axis=1)

plt.figure(figsize=(8, 8))

sns.heatmap(confusion_matrix(y_true, y_pred), cmap='Blues', annot=True, fmt='d')
plt.show()

print(classification_report(y_true, y_pred))

"""Model-2"""

# Define the modified LeNet-5 model for CIFAR-10
model = models.Sequential([
    layers.Input(shape=(32, 32, 3)),  # Adjusted to the correct input shape
    layers.Conv2D(6, kernel_size=(3, 3), activation='tanh', padding='same'),  # Padding added to maintain dimension
    layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2)),
    layers.Conv2D(16, kernel_size=(3, 3), activation='tanh'),
    layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2)),
    layers.Conv2D(120, kernel_size=(3, 3), activation='tanh'),
    layers.Flatten(),
    layers.Dense(84, activation='tanh'),
    layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Assuming X_train, y_train, X_test, and y_test are already loaded and preprocessed for CIFAR-10
# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=128, validation_data=(X_test, y_test))

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)
print("Test accuracy:", test_acc)

plt.figure(figsize=(12, 4))
plt.subplot(121)
plt.plot(history.history['loss'], label='training')
plt.plot(history.history['val_loss'], label='validation')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()

plt.subplot(122)
plt.plot(history.history['accuracy'], label='training')
plt.plot(history.history['val_accuracy'], label='validation')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend()

plt.show()

y_true = np.argmax(y_test, axis=1)
y_pred = np.argmax(model.predict(X_test), axis=1)

plt.figure(figsize=(8, 8))

sns.heatmap(confusion_matrix(y_true, y_pred), cmap='Blues', annot=True, fmt='d')
plt.show()

print(classification_report(y_true, y_pred))

print(classification_report(y_true, y_pred))

"""Model-3"""

# Define a more appropriate LeNet-5 model for CIFAR-10
model = models.Sequential([
    layers.Input(shape=(32, 32, 3)),
    layers.Conv2D(6, kernel_size=(7, 7), activation='tanh', padding='same'),  # Reduce kernel size, use 'same' padding
    layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2)),
    layers.Conv2D(16, kernel_size=(7, 7), activation='tanh', padding='same'),  # Use 'same' padding to maintain size
    layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2)),
    layers.Conv2D(120, kernel_size=(7, 7), activation='tanh', padding='same'),  # Reduce kernel size, use 'same' padding
    layers.Flatten(),
    layers.Dense(84, activation='tanh'),
    layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Assuming X_train, y_train, X_test, and y_test are already loaded and preprocessed for CIFAR-10
# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=128, validation_data=(X_test, y_test))

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)
print("Test accuracy:", test_acc)

plt.figure(figsize=(12, 4))
plt.subplot(121)
plt.plot(history.history['loss'], label='training')
plt.plot(history.history['val_loss'], label='validation')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()

plt.subplot(122)
plt.plot(history.history['accuracy'], label='training')
plt.plot(history.history['val_accuracy'], label='validation')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend()

plt.show()

y_true = np.argmax(y_test, axis=1)
y_pred = np.argmax(model.predict(X_test), axis=1)

plt.figure(figsize=(8, 8))

sns.heatmap(confusion_matrix(y_true, y_pred), cmap='Blues', annot=True, fmt='d')
plt.show()

print(classification_report(y_true, y_pred))

"""Model 4"""

from tensorflow.keras import models, layers, regularizers

# Define a more advanced CNN model for CIFAR-10
model = models.Sequential([
    layers.Input(shape=(32, 32, 3)),

    # First Convolutional Block
    layers.ZeroPadding2D(padding=(2, 2)),
    layers.Conv2D(32, kernel_size=(5, 5), activation='relu'),
    layers.BatchNormalization(),
    layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2)),

    # Second Convolutional Block
    layers.ZeroPadding2D(padding=(2, 2)),
    layers.Conv2D(64, kernel_size=(5, 5), activation='relu'),
    layers.BatchNormalization(),
    layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2)),

    # Third Convolutional Block
    layers.ZeroPadding2D(padding=(1, 1)),
    layers.Conv2D(128, kernel_size=(3, 3), activation='relu'),
    layers.BatchNormalization(),

    # Flattening followed by Dense Layers
    layers.Flatten(),
    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Assuming X_train, y_train, X_test, and y_test are already loaded and preprocessed for CIFAR-10
# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=128, validation_data=(X_test, y_test))

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)
print("Test accuracy:", test_acc)

plt.figure(figsize=(12, 4))
plt.subplot(121)
plt.plot(history.history['loss'], label='training')
plt.plot(history.history['val_loss'], label='validation')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()

plt.subplot(122)
plt.plot(history.history['accuracy'], label='training')
plt.plot(history.history['val_accuracy'], label='validation')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend()

plt.show()

y_true = np.argmax(y_test, axis=1)
y_pred = np.argmax(model.predict(X_test), axis=1)

plt.figure(figsize=(8, 8))

sns.heatmap(confusion_matrix(y_true, y_pred), cmap='Blues', annot=True, fmt='d')
plt.show()

print(classification_report(y_true, y_pred))

"""Model 5"""

from tensorflow.keras import models, layers, regularizers

# Define a CNN model for CIFAR-10 with consistent 7x7 kernel size
model = models.Sequential([
    layers.Input(shape=(32, 32, 3)),

    # First Convolutional Block
    layers.ZeroPadding2D(padding=(3, 3)),  # Maintain dimensionality with 7x7 kernel
    layers.Conv2D(32, kernel_size=(7, 7), activation='relu'),
    layers.BatchNormalization(),
    layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2)),

    # Second Convolutional Block
    layers.ZeroPadding2D(padding=(3, 3)),  # Apply padding to maintain output size
    layers.Conv2D(64, kernel_size=(7, 7), activation='relu'),
    layers.BatchNormalization(),
    layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2)),

    # Third Convolutional Block
    layers.ZeroPadding2D(padding=(3, 3)),  # This padding helps to maintain the 7x7 conv
    layers.Conv2D(128, kernel_size=(7, 7), activation='relu'),
    layers.BatchNormalization(),

    # Flattening followed by Dense Layers
    layers.Flatten(),
    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Assuming X_train, y_train, X_test, and y_test are already loaded and preprocessed for CIFAR-10
# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=128, validation_data=(X_test, y_test))

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)
print("Test accuracy:", test_acc)

plt.figure(figsize=(12, 4))
plt.subplot(121)
plt.plot(history.history['loss'], label='training')
plt.plot(history.history['val_loss'], label='validation')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()

plt.subplot(122)
plt.plot(history.history['accuracy'], label='training')
plt.plot(history.history['val_accuracy'], label='validation')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend()

plt.show()

print(classification_report(y_true, y_pred))

"""Model 6"""

model = models.Sequential([
    layers.Input(shape=(32, 32, 3)),
    layers.ZeroPadding2D(padding=(3, 3)),
    layers.Conv2D(32, kernel_size=(7, 7), activation='relu'),
    layers.Conv2D(32, kernel_size=(7, 7), activation='relu'),
    layers.BatchNormalization(),
    layers.AveragePooling2D(pool_size=(2, 2)),
    layers.ZeroPadding2D(padding=(3, 3)),
    layers.Conv2D(64, kernel_size=(7, 7), activation='relu'),
    layers.Conv2D(64, kernel_size=(7, 7), activation='relu'),
    layers.BatchNormalization(),
    layers.AveragePooling2D(pool_size=(2, 2)),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Assuming X_train, y_train, X_test, and y_test are already loaded and preprocessed for CIFAR-10
# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=128, validation_data=(X_test, y_test))

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)
print("Test accuracy:", test_acc)

plt.figure(figsize=(12, 4))
plt.subplot(121)
plt.plot(history.history['loss'], label='training')
plt.plot(history.history['val_loss'], label='validation')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()

plt.subplot(122)
plt.plot(history.history['accuracy'], label='training')
plt.plot(history.history['val_accuracy'], label='validation')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend()

plt.show()

print(classification_report(y_true, y_pred))

"""Model 7"""

model2 = models.Sequential([
    layers.Input(shape=(32, 32, 3)),
    layers.ZeroPadding2D(padding=(2, 2)),
    layers.Conv2D(32, kernel_size=(5, 5), activation='relu'),
    layers.Conv2D(32, kernel_size=(5, 5), activation='relu'),
    layers.BatchNormalization(),
    layers.AveragePooling2D(pool_size=(2, 2)),
    layers.ZeroPadding2D(padding=(2, 2)),
    layers.Conv2D(64, kernel_size=(5, 5), activation='relu'),
    layers.Conv2D(64, kernel_size=(5, 5), activation='relu'),
    layers.BatchNormalization(),
    layers.AveragePooling2D(pool_size=(2, 2)),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Assuming X_train, y_train, X_test, and y_test are already loaded and preprocessed for CIFAR-10
# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=128, validation_data=(X_test, y_test))

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)
print("Test accuracy:", test_acc)

plt.figure(figsize=(12, 4))
plt.subplot(121)
plt.plot(history.history['loss'], label='training')
plt.plot(history.history['val_loss'], label='validation')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()

plt.subplot(122)
plt.plot(history.history['accuracy'], label='training')
plt.plot(history.history['val_accuracy'], label='validation')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend()

plt.show()

print(classification_report(y_true, y_pred))

"""Model 8"""

model = models.Sequential([
    layers.Input(shape=(32, 32, 3)),
    layers.ZeroPadding2D(padding=(2, 2)),
    layers.Conv2D(32, kernel_size=(5, 5), activation='tanh'),
    layers.Conv2D(32, kernel_size=(5, 5), activation='tanh'),
    layers.BatchNormalization(),
    layers.AveragePooling2D(pool_size=(2, 2)),
    layers.ZeroPadding2D(padding=(2, 2)),
    layers.Conv2D(64, kernel_size=(5, 5), activation='tanh'),
    layers.Conv2D(64, kernel_size=(5, 5), activation='tanh'),
    layers.BatchNormalization(),
    layers.AveragePooling2D(pool_size=(2, 2)),
    layers.Flatten(),
    layers.Dense(128, activation='tanh'),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Assuming X_train, y_train, X_test, and y_test are already loaded and preprocessed for CIFAR-10
# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=128, validation_data=(X_test, y_test))

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)
print("Test accuracy:", test_acc)

plt.figure(figsize=(12, 4))
plt.subplot(121)
plt.plot(history.history['loss'], label='training')
plt.plot(history.history['val_loss'], label='validation')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()

plt.subplot(122)
plt.plot(history.history['accuracy'], label='training')
plt.plot(history.history['val_accuracy'], label='validation')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend()

plt.show()

print(classification_report(y_true, y_pred))

"""Outoff these all model i found model 4 is good according to the diagnostics"""

from tensorflow.keras import models, layers, regularizers

# Define a more advanced CNN model for CIFAR-10
model = models.Sequential([
    layers.Input(shape=(32, 32, 3)),

    # First Convolutional Block
    layers.ZeroPadding2D(padding=(2, 2)),
    layers.Conv2D(32, kernel_size=(5, 5), activation='relu'),
    layers.BatchNormalization(),
    layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2)),

    # Second Convolutional Block
    layers.ZeroPadding2D(padding=(2, 2)),
    layers.Conv2D(64, kernel_size=(5, 5), activation='relu'),
    layers.BatchNormalization(),
    layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2)),

    # Third Convolutional Block
    layers.ZeroPadding2D(padding=(1, 1)),
    layers.Conv2D(128, kernel_size=(3, 3), activation='relu'),
    layers.BatchNormalization(),

    # Flattening followed by Dense Layers
    layers.Flatten(),
    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Assuming X_train, y_train, X_test, and y_test are already loaded and preprocessed for CIFAR-10
# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=128, validation_data=(X_test, y_test))

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)
print("Test accuracy:", test_acc)

plt.figure(figsize=(12, 4))
plt.subplot(121)
plt.plot(history.history['loss'], label='training')
plt.plot(history.history['val_loss'], label='validation')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()

plt.subplot(122)
plt.plot(history.history['accuracy'], label='training')
plt.plot(history.history['val_accuracy'], label='validation')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend()

plt.show()

print(classification_report(y_true, y_pred))

cnn = models.Sequential([
    layers.Input(shape=(28, 28, 1)),
    layers.Conv2D(6, (3, 3)),
    layers.MaxPool2D(pool_size=(2, 2)),
    layers.Conv2D(12, (3, 3)),
    layers.MaxPool2D(pool_size=(2, 2)),
    layers.Conv2D(18, (3, 3)),
    layers.MaxPool2D(pool_size=(2, 2)),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.15),
    layers.Dense(10, activation='softmax')
])
cnn.summary()

y_true = np.argmax(y_test, axis=1)
y_pred = np.argmax(model.predict(X_test), axis=1)

plt.figure(figsize=(8, 8))

sns.heatmap(confusion_matrix(y_true, y_pred), cmap='Blues', annot=True, fmt='d')
plt.show()

from tensorflow.keras import models, layers, regularizers

# Define a more advanced CNN model for CIFAR-10
model = models.Sequential([
    layers.Input(shape=(32, 32, 3)),

    # First Convolutional Block
    layers.ZeroPadding2D(padding=(2, 2)),
    layers.Conv2D(32, kernel_size=(5, 5), activation='relu'),
    layers.BatchNormalization(),
    layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2)),

    # Second Convolutional Block
    layers.ZeroPadding2D(padding=(2, 2)),
    layers.Conv2D(64, kernel_size=(5, 5), activation='relu'),
    layers.BatchNormalization(),
    layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2)),

    # Third Convolutional Block
    layers.ZeroPadding2D(padding=(1, 1)),
    layers.Conv2D(128, kernel_size=(3, 3), activation='relu'),
    layers.BatchNormalization(),

    # Flattening followed by Dense Layers
    layers.Flatten(),
    layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Assuming X_train, y_train, X_test, and y_test are already loaded and preprocessed for CIFAR-10
# Train the model
history = model.fit(X_train, y_train, epochs=100, batch_size=128, validation_data=(X_test, y_test))

# Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)
print("Test accuracy:", test_acc)

plt.figure(figsize=(12, 4))
plt.subplot(121)
plt.plot(history.history['loss'], label='training')
plt.plot(history.history['val_loss'], label='validation')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()

plt.subplot(122)
plt.plot(history.history['accuracy'], label='training')
plt.plot(history.history['val_accuracy'], label='validation')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend()

plt.show()

print(classification_report(y_true, y_pred))

y_true = np.argmax(y_test, axis=1)
y_pred = np.argmax(model.predict(X_test), axis=1)

plt.figure(figsize=(8, 8))

sns.heatmap(confusion_matrix(y_true, y_pred), cmap='Blues', annot=True, fmt='d')
plt.show()

cnn = models.Sequential([
    layers.Input(shape=(32, 32, 3)),
    layers.Conv2D(32, (5, 5)),
    layers.MaxPool2D(pool_size=(2, 2)),
    layers.Conv2D(64, (5, 5)),
    layers.MaxPool2D(pool_size=(2, 2)),
    layers.Conv2D(128, (3, 3)),
    layers.MaxPool2D(pool_size=(2, 2)),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')
])
cnn.summary()

import cv2
def visualize_neuron_weights(model, layer_name, neuron_index, input_shape):
    # Ensure input_shape is a tuple of (height, width)
    assert isinstance(input_shape, tuple) and len(input_shape) == 2, "input_shape must be a tuple of (height, width)"

    # Extract weights for the specified layer
    weights, biases = model.get_layer(layer_name).get_weights()

    # Extract weights for the specified neuron
    neuron_weights = weights[:, neuron_index]
    # print(neuron_weights.shape)

    # Initial attempt to reshape weights to a square shape
    side_length = int(np.sqrt(len(neuron_weights)))
    if side_length * side_length != len(neuron_weights):
        # If weights cannot be reshaped into a perfect square, they need to be resized
        # First, resha pe into the closest square for initial visualization

        closest_square_side_length = int(np.sqrt(len(neuron_weights)))
        reshaped_weights = np.resize (neuron_weights, (closest_square_side_length**2))
        reshaped_weights = reshaped_weights.reshape((closest_square_side_length,closest_square_side_length))

        # Use OpenCV to resize (upsample or downsample) to match input_shape
        resized_weights = cv2.resize(reshaped_weights, input_shape, interpolation=cv2.INTER_LINEAR)
    else:
        # If weights can be perfectly squared, check if they match input_shape already
        reshaped_weights = np.reshape(neuron_weights, (side_length, side_length))
        if reshaped_weights.shape == input_shape:
            resized_weights = reshaped_weights
        else:
            # Resize to match input_shape if they don't match
            resized_weights = cv2.resize(reshaped_weights, input_shape, interpolation=cv2.INTER_LINEAR)

    # Visualization as heatmap
    plt.imshow(resized_weights, cmap='viridis')
    plt.title(f"Weights of Neuron {neuron_index} in {layer_name} (resized to input shape)")
    plt.colorbar()
    plt.show()

model.summary()

visualize_neuron_weights(model, 'dense_33', 9, input_shape=(28,28))

optim = Adam(learning_rate=0.001)
cnn.compile(optimizer=optim, loss='categorical_crossentropy', metrics=['accuracy'])

history = cnn.fit(X_train, y_train, epochs=35, batch_size=128, validation_data=(X_test, y_test))

plt.figure(figsize=(12, 4))
plt.subplot(121)
plt.plot(history.history['loss'], label='training')
plt.plot(history.history['val_loss'], label='validation')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()

plt.subplot(122)
plt.plot(history.history['accuracy'], label='training')
plt.plot(history.history['val_accuracy'], label='validation')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend()

plt.show()

print(classification_report(y_true, y_pred))

y_true = np.argmax(y_test, axis=1)
y_pred = np.argmax(model.predict(X_test), axis=1)

plt.figure(figsize=(8, 8))

sns.heatmap(confusion_matrix(y_true, y_pred), cmap='Blues', annot=True, fmt='d')
plt.show()

import cv2
def visualize_neuron_weights(model, layer_name, neuron_index, input_shape):
    # Ensure input_shape is a tuple of (height, width)
    assert isinstance(input_shape, tuple) and len(input_shape) == 2, "input_shape must be a tuple of (height, width)"

    # Extract weights for the specified layer
    weights, biases = model.get_layer(layer_name).get_weights()

    # Extract weights for the specified neuron
    neuron_weights = weights[:, neuron_index]
    # print(neuron_weights.shape)

    # Initial attempt to reshape weights to a square shape
    side_length = int(np.sqrt(len(neuron_weights)))
    if side_length * side_length != len(neuron_weights):
        # If weights cannot be reshaped into a perfect square, they need to be resized
        # First, resha pe into the closest square for initial visualization

        closest_square_side_length = int(np.sqrt(len(neuron_weights)))
        reshaped_weights = np.resize (neuron_weights, (closest_square_side_length**2))
        reshaped_weights = reshaped_weights.reshape((closest_square_side_length,closest_square_side_length))

        # Use OpenCV to resize (upsample or downsample) to match input_shape
        resized_weights = cv2.resize(reshaped_weights, input_shape, interpolation=cv2.INTER_LINEAR)
    else:
        # If weights can be perfectly squared, check if they match input_shape already
        reshaped_weights = np.reshape(neuron_weights, (side_length, side_length))
        if reshaped_weights.shape == input_shape:
            resized_weights = reshaped_weights
        else:
            # Resize to match input_shape if they don't match
            resized_weights = cv2.resize(reshaped_weights, input_shape, interpolation=cv2.INTER_LINEAR)

    # Visualization as heatmap
    plt.imshow(resized_weights, cmap='viridis')
    plt.title(f"Weights of Neuron {neuron_index} in {layer_name} (resized to input shape)")
    plt.colorbar()
    plt.show()

model.summary()

visualize_neuron_weights(model, 'dense_33', 9, input_shape=(28,28))

optim = Adam(learning_rate=0.0001)
cnn.compile(optimizer=optim, loss='categorical_crossentropy', metrics=['accuracy'])

history = cnn.fit(X_train, y_train, epochs=50, batch_size=128, validation_data=(X_test, y_test))

